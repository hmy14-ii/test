{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae681ef",
   "metadata": {},
   "source": [
    "## STA130 Homework 03 \n",
    "\n",
    "Please see the course [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) for the list of topics covered in this homework assignment, and a list of topics that might appear during ChatBot conversations which are \"out of scope\" for the purposes of this homework assignment (and hence can be safely ignored if encountered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf135c01",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Introduction</u></summary>\n",
    "\n",
    "### Introduction\n",
    "    \n",
    "A reasonable characterization of STA130 Homework is that it simply defines a weekly reading comprehension assignment. \n",
    "Indeed, STA130 Homework essentially boils down to completing various understanding confirmation exercises oriented around coding and writing tasks.\n",
    "However, rather than reading a textbook, STA130 Homework is based on ChatBots so students can interactively follow up to clarify questions or confusion that they may still have regarding learning objective assignments.\n",
    "\n",
    "> Communication is a fundamental skill underlying statistics and data science, so STA130 Homework based on ChatBots helps practice effective two-way communication as part of a \"realistic\" dialogue activity supporting underlying conceptual understanding building. \n",
    "\n",
    "It will likely become increasingly tempting to rely on ChatBots to \"do the work for you\". But when you find yourself frustrated with a ChatBots inability to give you the results you're looking for, this is a \"hint\" that you've become overreliant on the ChatBots. Your objective should not be to have ChatBots \"do the work for you\", but to use ChatBots to help you build your understanding so you can efficiently leverage ChatBots (and other resources) to help you work more efficiently.<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Instructions</u></summary>\n",
    "\n",
    "### Instructions\n",
    "    \n",
    "1. Code and write all your answers (for both the \"Pre-lecture\" and \"Post-lecture\" HW) in a python notebook (in code and markdown cells) \n",
    "    \n",
    "> It is *suggested but not mandatory* that you complete the \"Pre-lecture\" HW prior to the Monday LEC since (a) all HW is due at the same time; but, (b) completing some of the HW early will mean better readiness for LEC and less of a \"procrastentation cruch\" towards the end of the week...\n",
    "    \n",
    "2. Paste summaries of your ChatBot sessions (including link(s) to chat log histories if you're using ChatGPT) within your notebook\n",
    "    \n",
    "> Create summaries of your ChatBot sessions by using concluding prompts such as \"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\" or, \"Please provide me with the final working verson of the code that we created together\"\n",
    "    \n",
    "3. Save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Prompt Engineering?</u></summary>\n",
    "    \n",
    "### Prompt Engineering? \n",
    "    \n",
    "The questions (as copy-pasted prompts) are designed to initialize appropriate ChatBot conversations which can be explored in the manner of an interactive and dynamic textbook; but, it is nonetheless **strongly recommendated** that your rephrase the questions in a way that you find natural to ensure a clear understanding of the question. Given sensible prompts the represent a question well, the two primary challenges observed to arise from ChatBots are \n",
    "\n",
    "1. conversations going beyond the intended scope of the material addressed by the question; and, \n",
    "2. unrecoverable confusion as a result of sequential layers logial inquiry that cannot be resolved. \n",
    "\n",
    "In the case of the former (1), adding constraints specifying the limits of considerations of interest tends to be helpful; whereas, the latter (2) is often the result of initial prompting that leads to poor developments in navigating the material, which are likely just best resolve by a \"hard reset\" with a new initial approach to prompting.  Indeed, this is exactly the behavior [hardcoded into copilot](https://answers.microsoft.com/en-us/bing/forum/all/is-this-even-normal/0b6dcab3-7d6c-4373-8efe-d74158af3c00)...\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7075ccb5",
   "metadata": {},
   "source": [
    "\n",
    "### Marking Rubric (which may award partial credit) \n",
    "\n",
    "- [0.1 points]: All relevant ChatBot summaries [including link(s) to chat log histories if you're using ChatGPT] are reported within the notebook\n",
    "- [0.2 points]: Assignment completion confirmed by visual submission for \"2\" \n",
    "- [0.3 points]: Evaluation of written communication for \"3\" \n",
    "- [0.1 points]: Correct answers for \"4\"\n",
    "- [0.3 points]: Evidence of meaningful activity for \"6\"\n",
    "\n",
    "<!-- - [0.1 points]: Assignment completion confirmed by ChatBot interaction summaries for \"5\" -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a945f7c",
   "metadata": {},
   "source": [
    "### \"Pre-lecture\" HW [*completion prior to next LEC is suggested but not mandatory*]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e78c77",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. Use *fig.add_[h/v]line()* and *fig.add_[h/v]rect()* to mark, respspectively, location (mean and median) and scale (range, interquartile range, and a range defined by two standard deviations away from the mean in both directions) of *flipper_length_mm* for each _species_ onto _plotly_ histograms of *flipper_length_mm* for each _species_ in the penguins dataset<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "**Time Management Warning**: it takes a long time to make a figure, whether you're working with a ChatBot, or building it from scratch based on trial and error changes with your code. ChatBots remove the need to understand the detailed nuances of data visualization library arguments and construction procedures. But after you've passed the 30 minute range of effort working with your ChatBot for this problem to try to get what you want, then your only options are to start a new session and hope for a smoother experience based on improved clarity of your directions, or submit what you have along with a brief note highlighting the duration in your chatlog history where your efforts to make progress did not produce the desired outcome.\n",
    "\n",
    "> The code referenced above [*fig.add_[h/v]line()*](https://plotly.com/python/horizontal-vertical-shapes/) and [*fig.add_[h/v]rect()*](https://plotly.com/python/line-charts/) refer to `fig.add_hline()` and `fig.add_hline()` and `fig.add_hrect()` and `fig.add_vrect()` which overly lines rectangles onto a figure from different orientation perspectives.\n",
    ">\n",
    "> - _There are several considerations in this problem..._\n",
    ">     - _The histograms can be on the same figure, on separate figures, or separated into different panels in the same figure_\n",
    ">     - _The elements within a figure should be well annotated, probobably using a so-called legend to help make sure annotations don't overlap each other and are clear and readible_\n",
    "> - _There are several ways to approach this problem..._\n",
    ">     - _You will likely be very pleased when you run the code returned to you as the result of pasting this question in as a prompt into a ChatBot session; but, you will also likely need to interact with the ChatBot to ask for adjustments to the code which give a final satisfactory figure (and this is the recommended approach to get the experience this problem intends you to have)_\n",
    ">     - _**When using a ChatBot, if the code provided by your ChatBot results in an error, show the error to your ChatBot and iterate this process with the adjusted \"fixed\" code provided by the ChatBot... this process usually converges some something workable that's pretty close to what you were going for**_\n",
    ">     - <u>**And don't forget, a ChatBot can explain what how code it provides works, if you ask it to...**</u>\n",
    ">     - _You could alternatively figure out how to code this plot up for yourself by looking at the provided documentation links and perhaps using some additional google searchers or ChatBot queries to help out with specific issues or examples; and, if you end up interested in figuring out a little more how the code works that's great and definitely feel free to go ahead and do so, but at this stage the point of this problem is to understand the general ideas of figures themselves as opposed to being an expert about the code that generated them_\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cda70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is ChatGPT's code with my explanations\n",
    "#import the necessary libraries\n",
    "import pandas as pd #imports the pandas library to load csv for data manipulation\n",
    "import plotly.express as px #imports the plotly express library to be able to create graphs out of data\n",
    "import plotly.graph_objects as go #imports plotly's graph objects library to be able to customize graphs with fig.add\n",
    "\n",
    "#load penguin dataset\n",
    "penguins = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\") #Reads the dataset from the url to a pandas dataframe we named penguins\n",
    "penguins = penguins.dropna(subset=['flipper_length_mm', 'species']) #Drops rows with missing data in the flipper_length_mm and species columns to ensure the data is clean\n",
    "\n",
    "#create a function to add annotations\n",
    "def add_annotations(fig, data, species): #function will take a plotly graph(fig), the dataset of an animaml species (data), and the species name (species)\n",
    "    mean = data['flipper_length_mm'].mean() #searches in the dataset(data) to find the values of column 'flipper_length_mm' and caluculate the mean\n",
    "    median = data['flipper_length_mm'].median() #does the same as the line above but calculates the mean in the values of the 'flipper_length_mm' column\n",
    "    std = data['flipper_length_mm'].std() #does the same as the line above but calculates the standard deviation\n",
    "    q1 = data['flipper_length_mm'].quantile(0.25) #does the same as the line above but calculates the first quartile\n",
    "    q3 = data['flipper_length_mm'].quantile(0.75) #does the same as the line above but calculates the third quartile\n",
    "    range_min = data['flipper_length_mm'].min() #does the same as the line above but calculates the lowest flipper length\n",
    "    range_max = data['flipper_length_mm'].max() #does the same as the line above but calculates the highest flipper length\n",
    "    \n",
    "    # Add lines for mean and median\n",
    "    fig.add_vline(x=mean, line=dict(color='black', dash='dash'), annotation_text='Mean', annotation_position='top right')\n",
    "    #Adds a vertical line for the mean. The color is set to blue, the line is dashed, and the line will say Mean on its top right\n",
    "    fig.add_vline(x=median, line=dict(color='white', dash='dot'), annotation_text='Median', annotation_position='top left')\n",
    "    #Adds a vertical line for the median. The color is set to green, the line is dotted, and the line will say Median on its top left\n",
    "    \n",
    "    # Add rectangles for range, interquartile range, and 2 standard deviations\n",
    "    fig.add_vrect(x0=range_min, x1=range_max, fillcolor=\"lightblue\", opacity=0.3, line_width=0, annotation_text='Range')\n",
    "    #Rectangle highlights overall range (from minimum to maximum) in light blue color\n",
    "    fig.add_vrect(x0=q1, x1=q3, fillcolor=\"orange\", opacity=0.3, line_width=0, annotation_text='IQR')\n",
    "    #Rectangle highlights interquartile range (from 1st quartile to 3rd quartile) in orange color\n",
    "    fig.add_vrect(x0=mean-2*std, x1=mean+2*std, fillcolor=\"purple\", opacity=0.2, line_width=0, annotation_text='2 std dev')\n",
    "    #Rectangle highlights in purple color a range that is a standard deviation away from the mean in both directions\n",
    "    \n",
    "    return fig #function will return the new and improved plotly graph with the annotations made with fig.add\n",
    "\n",
    "#create histograms\n",
    "species_list = penguins['species'].unique() #makes a list of unique species in 'species' column\n",
    "\n",
    "for species in species_list:\n",
    "    species_data = penguins[penguins['species'] == species] #for each species in the list, a dataset will be made including data only from that species\n",
    "    \n",
    "    # Create histogram with 20 bins\n",
    "    fig = px.histogram(species_data, x='flipper_length_mm', nbins=20, title=f'Flipper Length Distribution for {species}')\n",
    "    \n",
    "    # Add statistical annotations\n",
    "    fig = add_annotations(fig, species_data, species)\n",
    "    \n",
    "    # Display the figure\n",
    "    fig.show(renderer=\"png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29540adc",
   "metadata": {},
   "source": [
    "#### 2. Transition your ChatBot session from the previous problem to repeat the previous problem, but this time using _seaborn_ **kernel density estimation** (KDE) plots to produce the desired figures organized in row of three plots<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "**Time Management Warning**: it takes a long time to make a figure, whether you're working with a ChatBot, or building it from scratch based on trial and error changes with your code. ChatBots remove the need to understand the detailed nuances of data visualization library arguments and construction procedures. But after you've passed the 30 minute range of effort working with your ChatBot for this problem to try to get what you want, then your only options are to start a new session and hope for a smoother experience based on improved clarity of your directions, or submit what you have along with a brief note highlighting the duration in your chatlog history where your efforts to make progress did not produce the desired outcome.\n",
    "    \n",
    "> The `seaborn` library extends `matplotlib` so [_ax.axhspan(...)_](https://matplotlib.org/stable/gallery/subplots_axes_and_figures/axhspan_demo.html#sphx-glr-gallery-subplots-axes-and-figures-axhspan-demo-py) or [_ax.fill_between(...)_](https://matplotlib.org/stable/gallery/lines_bars_and_markers/span_regions.html) from `matplotlib` could be combined with the [_seaborn_ KDE plot](https://seaborn.pydata.org/generated/seaborn.kdeplot.html)... this might be something to share with your ChatBot if it [tries to keep using _plotly_ or a KDE function rather than a _plotly_](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk3/GPT/SLS/00001_gpt3p5_plotlyseaborn_plotting.md) plotting functionality...\n",
    "> \n",
    "> - _When using a ChatBot, if the code provided by your ChatBot results in an error, show the error to your ChatBot and iterate this process with the adjusted \"fixed\" code provided by the ChatBot... this process usually converges some something workable that's pretty close to what you were going for_\n",
    "> - _**Also consider the ways that you might be able to split up the instructions for the ChatBot into multiple steps, creating a sequence of additional directions and extensions along the way as you mold the figure more and more into a form increasingly matching your desired output.**_\n",
    "> - And don't forget, a ChatBot can explain what how code it provides works, if you ask it to...\n",
    "> \n",
    "> The technical details of the following are beyond the scope of STA130, but if you were interested, you could very briefly examine the [_seaborn_ themes](https://seaborn.pydata.org/tutorial/aesthetics.html) based on `sns.set_style()` and `sns.set_theme()` and [_colors_](https://seaborn.pydata.org/tutorial/color_palettes.html) based on the `palette` parameter, e.g.,\n",
    "> \n",
    "> ```python\n",
    "> sns.set_style(\"whitegrid\") # sns.set_style(\"dark\")\n",
    "> # `sns.set_palette()` exists but functions often access and set that directly\n",
    "> sns.boxplot(..., hue='column', palette=\"colorblind\") \n",
    "> ```    \n",
    "> \n",
    "> and then attempt to interact with the ChatBot to change the coloring of the figure to something that you like and looks more clear to you... \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a80350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT code with my explanations\n",
    "#import necessary libraries\n",
    "import pandas as pd #pandas to load and manipulate datasets\n",
    "import matplotlib.pyplot as plt #matplotlib is a library that is used to create visuals in python\n",
    "import seaborn as sns #seaborn is a library built on matplotlib that makes it easy to create graphs \n",
    "import numpy as np #numpy for the math functions to operate large matrices and arrays\n",
    "\n",
    "#load the penguins dataset\n",
    "penguins = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\") #reads the data from url and saves it on pandas dataframe we named pandas\n",
    "penguins = penguins.dropna(subset=['flipper_length_mm', 'species']) #drops any rows with data missing in 'flipper_length_mm' and 'species' columns\n",
    "\n",
    "#create a function that adds annotations\n",
    "def add_kde_annotations(ax, data): #this function only take two parameters: the subplot (ax), and the dataset\n",
    "    mean = data.mean() #finds the mean in the numerical data \n",
    "    median = np.median(data) #finds the median in the numerical data\n",
    "    std = data.std() #finds the standard deviation in the numerical data\n",
    "    q1 = np.percentile(data, 25) #finds the first quartile in the numerical data\n",
    "    q3 = np.percentile(data, 75) #finds the third quartile in the numerical data\n",
    "    range_min = data.min() #finds the minimum value in the numerical data\n",
    "    range_max = data.max() #finds the maximum value in the numerical data\n",
    "\n",
    "    # Add lines for mean and median\n",
    "    ax.axvline(mean, color='black', linestyle='--', label='Mean') #creates a black and dashed vertical line where the mean is on the x-axis\n",
    "    ax.axvline(median, color='white', linestyle=':', label='Median') #creates a white and dotted vertical line where the median is on the x-axis\n",
    "\n",
    "    # Add shaded regions for range, IQR, and 2 standard deviations\n",
    "    ax.axvspan(range_min, range_max, color='lightblue', alpha=0.3, label='Range') #creates a light blue rectangle in the range from the minumum value to the maximum value\n",
    "    ax.axvspan(q1, q3, color='orange', alpha=0.3, label='IQR') #creates an orange rectangle in the range from the first quartile to the third quartile\n",
    "    ax.axvspan(mean-2*std, mean+2*std, color='purple', alpha=0.2, label='2 std dev') #creates a purple rectangle that spans one standard deviation from the mean in both directions\n",
    "\n",
    "#generate KDE plots for each species\n",
    "species_list = penguins['species'].unique() #creates a list with one of each unique species\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6)) #sets the figure to have three subplots in one row, with a specified size of 18x6 inches.\n",
    "\n",
    "for i, species in enumerate(species_list): #loops though each species in the species list with i as the index\n",
    "    species_data = penguins[penguins['species'] == species]['flipper_length_mm'] #creates a dataset for each species flipper length including data only for that species\n",
    "    \n",
    "    sns.kdeplot(species_data, ax=axes[i], fill=True) #creates a kde graph for each species with i telling the function which subplot (ax) to plot on\n",
    "    axes[i].set_title(f'Flipper Length KDE for {species}') #in the subplot creates a title for the graph\n",
    "    axes[i].set_xlabel('Flipper Length (mm)') #in the subplot creates a title for x-axis\n",
    "    \n",
    "    # Add statistical annotations\n",
    "    add_kde_annotations(axes[i], species_data)\n",
    "    #creates a legend for the annotations\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout() #evenly spaces the subplots\n",
    "plt.show() #renders and shows the plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab89605",
   "metadata": {},
   "source": [
    "#### 3. Search online for some images of **box plots**, **histograms**, and **kernel density estimators** (perhaps for the same data set); describe to a ChatBot what you think the contrasting descriptions of these three \"data distribution\" visualization methods are; and then see if the ChatBot agrees and what \"pros and cons\" list of these three \"data distribution\" visualization methods your ChatBot can come up with; finally, describe your preference for one or the other and your rationale for this preference<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> The details of the [\"kernel\"](https://en.wikipedia.org/wiki/Kernel_density_estimation) and how it works in [kernel density estimation](https://plotly.com/python/violin/#split-violin-plot) are beyond the scope of STA130; but, there is typically a so-called \"bandwidth\" **argument** (e.g., `bw_adjust` in [_seaborn_](https://stackoverflow.com/questions/37932283/confusion-with-bandwidth-on-seaborns-kdeplot)) that \"controls the width of the kernel\" which is analgous to the \"number of bins parameter\" of a histogram (e.g., `nbins` in [_plotly_](https://www.google.com/search?client=safari&rls=en&q=plotly+nbins&ie=UTF-8&oe=UTF-8))  <!-- 4. Report on your preferences between `plotly` and `seaborn` in terms of usability and the general visual aestetics -->\n",
    "> \n",
    "> _Don't forget to ask for summaries of your ChatBot session(s) and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatGPT)_\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6075f",
   "metadata": {},
   "source": [
    "My definitions:\n",
    "\n",
    "- Box plot: It is a horizontal line with five vertical lines on it. The lines of the very ends are the maximum and minimum values and the line in the middle is the median value. The second and fourth lines are the 1st and 3rd quartile and they form a box that represents the interquartile range. \n",
    "- Histogram: A histogram displays the spread of data in a series of bars called bins. Each bin represents a range of numbers. The bar's height represents the frequency of data points in the range of numbers.\n",
    "- KDE: A histogram but instead of having bins, it displays the spread of data as a curve; it's almost like the graph has an infinite amount of bins.\n",
    "\n",
    "ChatGPT's definitions:\n",
    "\n",
    "- Box Plot: A box plot displays the distribution of data using a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. The box represents the interquartile range (IQR) between Q1 and Q3, with lines (whiskers) extending to the minimum and maximum values.\n",
    "- Histogram: A histogram visualizes data distribution using bars, where each bar represents a range of values (bins) and the height indicates the frequency of data points within that range. The bins cover the entire dataset, typically with equal widths.\n",
    "- KDE: A KDE provides a smooth curve that represents the distribution of data, acting as a continuous version of a histogram. It estimates the probability density function, offering a refined view of data spread without discrete bins.\n",
    "\n",
    "ChatGPT's pros and cons:\n",
    "\n",
    "Box Plot\n",
    "- Pros:\n",
    "    - Concise Summary: Effectively summarizes key statistical measures (median, quartiles, and range) in a compact visual format.\n",
    "    - Outlier Detection: Easily highlights outliers and provides insights into the distribution shape and variability.\n",
    "- Cons:\n",
    "    - Limited Detail: Does not show the distribution of values within the quartiles, so it can obscure data trends.\n",
    "    - Less Intuitive for Some: May be less familiar to some audiences, making it harder for them to interpret.\n",
    "    \n",
    "Histogram\n",
    "- Pros:\n",
    "    - Frequency Representation: Clearly displays the frequency of data points across specified ranges, making it easy to identify trends and patterns.\n",
    "    - Flexibility in Bin Width: Users can adjust bin sizes to see different levels of detail in the data distribution.\n",
    "- Cons:\n",
    "    - Bin Sensitivity: The choice of bin size can significantly affect the interpretation of the data, potentially leading to misleading conclusions.\n",
    "    - Not Ideal for Small Samples: May not effectively represent distributions with small sample sizes, as the bars can appear sparse or uneven.\n",
    "    \n",
    "Kernel Density Estimate (KDE)\n",
    "- Pros:\n",
    "    - Smooth Representation: Provides a continuous and smooth representation of the data distribution, making it easier to visualize patterns.\n",
    "    - Detail in Data Shape: Captures underlying data distribution characteristics that histograms may miss, such as multimodal distributions.\n",
    "- Cons:\n",
    "    - Bandwidth Sensitivity: The choice of bandwidth (smoothing parameter) can greatly affect the appearance and interpretation of the KDE, leading to over-smoothing or under-smoothing.\n",
    "    - Less Intuitive: May be harder for some audiences to interpret compared to histograms or box plots, especially if they are unfamiliar with the concept.\n",
    "    \n",
    "My favorite: I like the histogram because it shows the distribution of data in a way that is better than the other two choices. It is better than the box plot because it is more detailed with how often data appears in more specific ranges. It is better than the kde because the kde could give a less accurate representation of the data by over smoothing the curve. Either way, I don't really have too much of a preference since they can all give me the mean, median, iqr, and std."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8cc949",
   "metadata": {},
   "source": [
    "#### 4. Run the code below and look at the resulting figure of distrubutions and then answer the following questions\n",
    "\n",
    "1. Which datasets have similar means and similar variances\n",
    "2. Which datasets have similar means but quite different variances\n",
    "3. Which datasets have similar variances but quite different means\n",
    "4. Which datasets have quite different means and quite different variances\n",
    "    \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> Can you answer these questions immediately? If not, first review what the basic ideas of **sample means** and **sample standard deviations** (and **sample variances**) are. Their mathematical definitions are given below, and are useful for understanding the intuition of these concepts in terms of \"averages\" of things, like \"observations\" or \"squared differences\" (and then perhaps square-rooted). But there are other ways to \"intuitively visually\" understand **sample means** and **sample standard deviations** (and **sample variances**) which a ChatBot would be able to discuss with you.\n",
    ">\n",
    "> - sample mean $\\displaystyle \\bar x = \\frac{1}{n}\\sum_{i=1}^n x_i$ \n",
    "> - sample variance $\\displaystyle s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar x)^2$\n",
    "> - sample standard deviation $\\displaystyle s = \\sqrt{s^2}$\n",
    ">\n",
    "> It's potentially maybe possible that you or a ChatBot could answer these questions by looking at the code that produced the data you're considering. But if you're trying to check and understand things that way, you should instead consider just calculate the statistics that answer the questions themselves...\n",
    "> - `np.mean(df.col)` or `df.col.mean()`\n",
    "> - `np.std(df.col, dof=1)` / `np.var(df.col, dof=1)` or `df.col.std(dof=1)` / `df.col.var(dof=1)`\n",
    ">\n",
    "> _If you are resorting to calculating the statistics that answer the questions, try to understand the answers after you have them... just getting the \"right\" answers kind of defeats the point of this exercise..._\n",
    ">\n",
    "> - The difference between trying to answer this question using the code that produced the data versus calculating the statistics from the data comes down to the difference between **parameters** and **statistics**, but this will be discussed in the lecture... in the meantime, howevever, if you're curious about this... you could consider prompting a ChatBot to explain the difference between **parameters** and **statistics**...\n",
    ">     - ... this would naturally lead to some discussion of the relationship between **populations** and **samples**, and from there it would only be a little further to start working to understand the relationship between **statistics** and **parameters** and how they connect to *populations* and *samples* (and hence each other)...    \n",
    "    \n",
    "</details>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c149e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "n = 1500\n",
    "data1 = stats.uniform.rvs(0, 10, size=n)\n",
    "data2 = stats.norm.rvs(5, 1.5, size=n)\n",
    "data3 = np.r_[stats.norm.rvs(2, 0.25, size=int(n/2)), stats.norm.rvs(8, 0.5, size=int(n/2))]\n",
    "data4 = stats.norm.rvs(6, 0.5, size=n)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=4)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=data1, name='A', nbinsx=30, marker=dict(line=dict(color='black', width=1))), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=data2, name='B', nbinsx=15, marker=dict(line=dict(color='black', width=1))), row=1, col=2)\n",
    "fig.add_trace(go.Histogram(x=data3, name='C', nbinsx=45, marker=dict(line=dict(color='black', width=1))), row=1, col=3)\n",
    "fig.add_trace(go.Histogram(x=data4, name='D', nbinsx=15, marker=dict(line=dict(color='black', width=1))), row=1, col=4)\n",
    "\n",
    "fig.update_layout(height=300, width=750, title_text=\"Row of Histograms\")\n",
    "fig.update_xaxes(title_text=\"A\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"B\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"C\", row=1, col=3)\n",
    "fig.update_xaxes(title_text=\"D\", row=1, col=4)\n",
    "fig.update_xaxes(range=[-0.5, 10.5])\n",
    "\n",
    "for trace in fig.data:\n",
    "    trace.xbins = dict(start=0, end=10)\n",
    "    \n",
    "# This code was produced by just making requests to Microsoft Copilot\n",
    "# https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk3/COP/SLS/0001_concise_makeAplotV1.md\n",
    "\n",
    "fig.show(renderer=\"png\") # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc34394",
   "metadata": {},
   "source": [
    "1. Which datasets have similar means and similar variances?\n",
    "    - Data 1 (A) and data 3 (C) have similar means; they are centered at around 5. The two datasets also have similar variances since they both have many values that are far away from the mean. \n",
    "2. Which datasets have similar means but quite different variances?\n",
    "    - Data 2 (B) and data 3 (C) have similar means but different variances. The mean for B is around 5 and so is the mean for C. However, because C is bimodal, the variance is larger due to it's split distribution. This is unlike B where most values cluster around 5 which leads to a lower variance. \n",
    "3. Which datasets have similar variances but quite different means?\n",
    "    - Data 1 (A) and data 3 (C) have similar variances since they both have many values that are far away from the mean. Although I said earlier that both of them have a mean of 5, the mean of 5 doesn't accurately describe C since it is bimodal and has two subgroups with means at around 2 and 8. This is unlike A which is uniformely distributed and has a mean at 5. \n",
    "4. Which datasets have quite different means and quite different variances\n",
    "    - Data 2 (B) and data 4 (D) have the most different means with values on B gathering around 5 and the values on D gathering closer to 6. The two datasets also have different variances with B having more variance in values than C where the values are held closer together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6279b8",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Continue now...?</u></summary>\n",
    "\n",
    "### Pre-lecture VS Post-lecture HW\n",
    "\n",
    "Feel free to work on the \"Post-lecture\" HW below if you're making good progress and want to continue: the next questions will just continue working on data visualization related topics, so, it's just a choice whether or not you want to work a head a little bit... \n",
    "\n",
    "- The previous suggestions regarding **parameters** versus **statistics** would be a very good thing to look at carefully in preparation for the upcoming lecture...\n",
    "    \n",
    "*The benefits of continue would are that (a) it might be fun to try to tackle the challenge of working through some problems without additional preparation or guidance; and (b) this is a very valable skill to be comfortable with; and (c) it will let you build experience interacting with ChatBots (and beginning to understand their strengths and limitations in this regard)... it's good to have sense of when using a ChatBot is the best way to figure something out, or if another approach (such as course provided resources or a plain old websearch for the right resourse) would be more effective*\n",
    "    \n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99c427",
   "metadata": {},
   "source": [
    "### \"Post-lecture\" HW [*submission along with \"Pre-lecture\" HW is due prior to next TUT*]\n",
    "\n",
    "#### 5. Start a new ChatBot session to explore the general relationship between the *mean* and *median* and \"right\" and \"left\" skewness (and why this is); what the following code does and how it works; and then explain (in your own words) the relationship between the *mean* and *median* and \"right\" and \"left\" skewness and what causes this, using and extending the code to demonstrate your explanation through a sequence of notebook cells.<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> You could start this session perhaps something like [this](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk3/GPT/SLS/00003_GPT3p5_meanVmedian.md)?\n",
    "> \n",
    "> _Don't forget to ask for summaries of your ChatBot session(s) and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatGPT)..._\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "  \n",
    "sample1 = stats.gamma(a=2,scale=2).rvs(size=1000) #generates a right skewed sample\n",
    "fig1 = px.histogram(pd.DataFrame({'data': sample1}), x=\"data\") #creates a histogram\n",
    "\n",
    "sample1_mean = sample1.mean() #finds mean of sample1\n",
    "sample1_median = np.quantile(sample1, [0.5]) #finds median of sample1\n",
    "\n",
    "sample2 = -stats.gamma(a=2,scale=2).rvs(size=1000) #negates sample1 data so it's now left skewed\n",
    "\n",
    "fig1.show(renderer=\"png\") #displays graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc4b0b4",
   "metadata": {},
   "source": [
    "Skewness is the asymmetry in the distribution of data. There is right skewnes and left skewness.\n",
    "\n",
    "Right Skewness: The data is mostly on the left side of the graph meaning the graph's longer tail is on the right side. In this case, the mean is usually greater than the median.\n",
    "\n",
    "Left Skewness: The data is mostly on the right side of the graph meaning the graph's longer tail is on the left side. In this case, the mean is usually less than the median.\n",
    "\n",
    "Causes of Skewness: It can be caused by outliers or extreme values. If one side has a really big value, it can pull the mean away from the median. Also, uneven distribution can inherently create skewness.\n",
    "\n",
    "Explanation of code:\n",
    "\n",
    "- stats.gamma(a=2, scale=2) generates a right-skewed distribution with shape parameter a=2.\n",
    "- px.histogram creates a histogram of the sample data.\n",
    "- The mean is calculated using sample1.mean(), and the median is found using np.quantile(sample1, [0.5]).\n",
    "- sample2 is created by negating the values of sample1, which results in a left-skewed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a430ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extension of code: ChatGPT code with my explanations\n",
    "# Importing necessary libraries\n",
    "import matplotlib.pyplot as plt #matplotlib is a library that is used to create visual plot graphs in python\n",
    "\n",
    "# Visualize the right-skewed distribution\n",
    "plt.figure(figsize=(14, 6)) #sets up the size of the figure where the two graphs will be \n",
    "\n",
    "# Histogram for right-skewed sample\n",
    "plt.subplot(1, 2, 1) #we set up one row and two column subplot, the third one indicates that we are working on the first column of subplot\n",
    "plt.hist(sample1, bins=30, color='skyblue', alpha=0.7, edgecolor='black') #creates a histogram with 30 bins from the first sample of data\n",
    "plt.axvline(sample1_mean, color='red', linestyle='dashed', linewidth=1, label='Mean') #creates a red dashed line that represents the mean\n",
    "plt.axvline(sample1_median, color='green', linestyle='dashed', linewidth=1, label='Median') #creates a green dashed line that represent the median\n",
    "plt.title('Right-Skewed Distribution') #adds a title\n",
    "plt.xlabel('Value') #adds a title for the x-axis\n",
    "plt.ylabel('Frequency') #adds a title for the y-axis\n",
    "plt.legend() #adds a legend\n",
    "\n",
    "# Histogram for left-skewed sample\n",
    "plt.subplot(1, 2, 2) #we set up one row and two column subplot, the third one indicates that we are working on the second column of subplot\n",
    "plt.hist(sample2, bins=30, color='salmon', alpha=0.7, edgecolor='black') #creates a histogram with 30 bins from the second sample of data\n",
    "plt.axvline(sample2.mean(), color='red', linestyle='dashed', linewidth=1, label='Mean') #creates a red dashed line that represents the mean\n",
    "plt.axvline(np.quantile(sample2, [0.5]), color='green', linestyle='dashed', linewidth=1, label='Median') #creates a green dashed line that represent the median\n",
    "plt.title('Left-Skewed Distribution') #adds a title\n",
    "plt.xlabel('Value') #adds a title for x-axis\n",
    "plt.ylabel('Frequency') #adds a title for y-axis\n",
    "plt.legend() #adds a legend\n",
    "\n",
    "plt.tight_layout() #evenly spaces the graphs\n",
    "plt.show() #displays the graphs\n",
    "\n",
    "#As shown by the graphs, in a right skewness or positive skewness, the mean is pulled to the right by the large values in the tail. This makes it greater than the median that stays to the left where most of the values are.\n",
    "#Also shown by the graphs, in a left skewness or negative skewness, the mean is pulled to the left by the small values in the tail. This makes it less than the median which stays to the right where most of the values are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff445fc",
   "metadata": {},
   "source": [
    "#### 6. Go find an interesting dataset and use summary statistics and visualizations to understand and demonstate some interesting aspects of the data<br>\n",
    "\n",
    "1. Your approach should likely follow what was suggested for the **Week 02 TUT Communication Activity from TUT**\n",
    "2. In the **Week 03 TUT Communication Activity from TUT** you will be put in groups and determine which group members dataset introduction will be presented by the group\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> A good place to browse datasets is [TidyTuesday](https://github.com/rfordatascience/tidytuesday/blob/master/README.md) as working with ChatBots to find unconventional and entertaining datasets is not particularly productive and only seems to end up with the datasets seen here and other (more interesting?) suggestions like [iris](https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv), [superheros](https://raw.githubusercontent.com/steview-d/superhero-dashboard/master/static/data/superheroData.csv), [hauntings](https://raw.githubusercontent.com/andreamoleri/Hauntings/main/hauntings.csv), [bigfoot](https://raw.githubusercontent.com/hannahramirez/BigfootVsUfos/main/bigfoot_mod.csv), [ufos](https://raw.githubusercontent.com/hannahramirez/BigfootVsUfos/main/ufo_mod.csv), [sharks](https://raw.githubusercontent.com/IbaiGallego/DataCleaning_SharkAttack/main/data/jaws.csv), [legos](https://raw.githubusercontent.com/seankross/lego/master/data-tidy/legosets.csv), [bees](https://gist.githubusercontent.com/bootshine2/ba15d3cb38e2ed31129aeca403405a12/raw/10949901cd8a6a75aa46c86b804c42ff410f929e/Bee%2520Colony%2520Loss.csv), [housing](https://raw.githubusercontent.com/slavaspirin/Toronto-housing-price-prediction/master/houses_edited.csv), and [gapminder](https://raw.githubusercontent.com/kirenz/datasets/master/gapminder.csv)\n",
    "> ```python\n",
    "> # Maybe something like this? Feel free to use this one \n",
    "> # if it strikes your fancy after look around a bit\n",
    "> import pandas as pd\n",
    "> df = pd.read_csv(\"https://raw.githubusercontent.com/manuelamc14/fast-food-Nutritional-Database/main/Tables/nutrition.csv\")\n",
    "> df # df.columns\n",
    "> ```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b518e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "import pandas as pd\n",
    "movies = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-07-30/summer_movies.csv')\n",
    "movies = movies.dropna(subset=['average_rating', 'title_type'])\n",
    "\n",
    "#make histograms\n",
    "import plotly.express as px\n",
    "\n",
    "def add_annotations(fig, data, medium):\n",
    "    mean = data['average_rating'].mean()\n",
    "    median = data['average_rating'].median()\n",
    "    std = data['average_rating'].std()\n",
    "    q1 = data['average_rating'].quantile(0.25)\n",
    "    q3 = data['average_rating'].quantile(0.75)\n",
    "    range_min = data['average_rating'].min()\n",
    "    range_max = data['average_rating'].max()\n",
    "    \n",
    "    fig.add_vline(x=mean, line=dict(color='blue', dash='dash'), annotation_text='Mean', annotation_position='bottom left')\n",
    "    fig.add_vline(x=median, line=dict(color='green', dash='dot'), annotation_text='Median', annotation_position='bottom right')\n",
    "    \n",
    "    fig.add_vrect(x0=range_min, x1=range_max, fillcolor=\"lightblue\", opacity=0.3, line_width=0, annotation_text='Range')\n",
    "    fig.add_vrect(x0=q1, x1=q3, fillcolor=\"orange\", opacity=0.3, line_width=0, annotation_text='IQR')\n",
    "    fig.add_vrect(x0=mean-2*std, x1=mean+2*std, fillcolor=\"purple\", opacity=0.2, line_width=0, annotation_text='2 std dev')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "medium_list = movies['title_type'].unique()\n",
    "\n",
    "for medium in medium_list:\n",
    "    movie_data = movies[movies['title_type'] == medium]\n",
    "    \n",
    "    fig = px.histogram(movie_data, x='average_rating', nbins=20, title=f'Distribution of Ratings for {medium}s')\n",
    "    fig = add_annotations(fig, movie_data, medium)\n",
    "    fig.show(renderer='png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73bb97",
   "metadata": {},
   "source": [
    "Analysis: We are dealing with nominal qualitative data when we compare the media types: movies, tvshows, and videos. We are dealing with continuous quantitative data when we view the average rating and discrete data when we view the counts for each range of ratings, represented by bins on the histogram. \n",
    "\n",
    "For movies, we notice that the data is skewed left which means that the mean is pulled to the left of the median because of extreme values on the longer tail on the left. In left-skewed data, the mean is usually less than the median, which indicates that the central tendency represented by the mean might not accurately reflect the typical value of the dataset, especially in the presence of outliers. This skewness can affect the results of statistical tests that assume normality, potentially leading to invalid conclusions if tests are applied without addressing the skewness. In a left-skewed distribution, the standard deviation may be larger due to extreme low values that increase variability. This can give a misleading impression of data spread, as most values are concentrated on the higher end. This is true as the std spread is very large for movies but the iqr suggests that most values stay close around a certain area meaning less variance.\n",
    "\n",
    "The same effects can be observed for tv shows where the data is skewed right, meaning that the mean is pulled to the right of the median because of extreme values on the longer tail on the right. In right-skewed data, the mean is usually greater than the median, indicating that the average may not accurately represent the real observed data, especially in the presence of outliers. This skewness can affect the results of statistical tests that assume normality, potentially leading to misleading conclusions. The standard deviation may be inflated due to extreme high values, giving a false impression of data spread, as most values cluster toward the lower end. For example, while the standard deviation might suggest a wide spread in movie ratings, the interquartile range (IQR) may show that most values are closely grouped, indicating less variability among the majority of observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f2749",
   "metadata": {},
   "source": [
    "#### 7. Watch the classic [Gapminder Video](https://www.youtube.com/watch?v=jbkSRLYSojo), then have a look at the [`plotly` version](https://plotly.com/python/animations/) and recreate the animation (perhaps after optionally exploring and changing the [style](https://plotly.com/python/templates/), if you wish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "df = px.data.gapminder()\n",
    "fig = px.scatter(df, x=\"gdpPercap\", y=\"lifeExp\", animation_frame=\"year\", animation_group=\"country\",\n",
    "           size=\"pop\", color=\"continent\", hover_name=\"country\",\n",
    "           log_x=True, size_max=55, range_x=[100,100000], range_y=[25,90])\n",
    "           \n",
    "fig.show(renderer=\"png\")\n",
    "\n",
    "#I'm not sure if taking the plotly version and checking it out here is what the task is but that is what I did"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c953507",
   "metadata": {},
   "source": [
    "#### 8. Provide a second version of the figure from the previous problem where you edit the `fig = px.scatter()` function from the Gapminder code so that `x` is \"percent change\", `y` is \"rank\", `size` is \"percent\", and `color`=\"sex\", `animation_frame` is \"year\", and `animation_group` and `hover_name` are \"name\". Then use `size_max=50`, `range_x=[-0.005,0.005])` and remove the `log_x=True` and `range_y` parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = pd.read_csv('https://raw.githubusercontent.com/hadley/data-baby-names/master/baby-names.csv')\n",
    "bn['name'] = bn['name']+\" \"+bn['sex'] # make identical boy and girl names distinct\n",
    "bn['rank'] = bn.groupby('year')['percent'].rank(ascending=False)\n",
    "bn = bn.sort_values(['name','year'])\n",
    "# the next three lines create the increaes or decrease in name prevalence from the last year \n",
    "bn['percent change'] = bn['percent'].diff()\n",
    "new_name = [True]+list(bn.name[:-1].values!=bn.name[1:].values)\n",
    "bn.loc[new_name,'percentage change'] = bn.loc[new_name,'percent'] \n",
    "bn = bn.sort_values('year')\n",
    "bn = bn[bn.percent>0.001] # restrict to \"common\" names\n",
    "\n",
    "fig = px.scatter(bn, x=\"percent change\", y=\"rank\", animation_frame=\"year\", animation_group=\"name\",\n",
    "                 size=\"percent\", color=\"sex\", hover_name=\"name\", size_max=50, range_x=[-0.005,0.005]) # range_y removed\n",
    "fig.update_yaxes(autorange='reversed') # this lets us put rank 1 on the top\n",
    "fig.show(renderer=\"png\") # USE `fig.show(renderer=\"png\")` FOR ALL GitHub and MarkUs SUBMISSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe842f2",
   "metadata": {},
   "source": [
    "#### 9. Have you reviewed the course [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?<br><br>\n",
    "  \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> _Just answering \"Yes\" or \"No\" or \"Somewhat\" or \"Mostly\" or whatever here is fine as this question isn't a part of the rubric; but, the midterm and final exams may ask questions that are based on the tutorial and lecture materials; and, your own skills will be limited by your familiarity with these materials (which will determine your ability to actually do actual things effectively with these skills... like the course project...)_\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b822c05",
   "metadata": {},
   "source": [
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0246ee5",
   "metadata": {},
   "source": [
    "ChatGPT logs:\n",
    "- https://chatgpt.com/share/66f25574-bca4-8013-9615-879ba0a0c6a3\n",
    "- https://chatgpt.com/share/66f255b7-861c-8013-b242-b221888e9705\n",
    "- https://chatgpt.com/share/66f255cb-5a40-8013-87c0-c8e45aff3c49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aab1be",
   "metadata": {},
   "source": [
    "# Recommended Additional Useful Activities [Optional]\n",
    "\n",
    "The \"Ethical Profesionalism Considerations\" and \"Current Course Project Capability Level\" sections below **are not a part of the required homework assignment**; rather, they are regular weekly guides covering (a) relevant considerations regarding professional and ethical conduct, and (b) the analysis steps for the STA130 course project that are feasible at the current stage of the course\n",
    "\n",
    "<br><details class=\"details-example\"><summary style=\"color:blue\"><u>Ethical Professionalism Considerations</u></summary>\n",
    "\n",
    "### Ethical Professionalism Considerations\n",
    "\n",
    "|![](https://handsondataviz.org/images/14-detect/gdp-baseline-merged-annotated.png)|\n",
    "|-|\n",
    "| |\n",
    "\n",
    "Mark Twain's statment that, \"There are lies, damn lies, and statistics\", reflects a general skepticism towards statistical analysis that has been reinforced through through popular books such as [How to Lie with Statistics](https://en.wikipedia.org/wiki/How_to_Lie_with_Statistics). One place \"statistics\" can be used to decieve is through misuse of charts.  As discussed [here](https://handsondataviz.org/how-to-lie-with-charts.html) and many other places, a primary tactic that can be used to give a misleading impression using a chart is the manipulation of axes or the addition of additional dimensions which distort the meaning of size. **What are the problems with the following graphs?**\n",
    "\n",
    "|![](https://images.ctfassets.net/jicu8fwm4fvs/260tj0wxTFCAlbf4yTzSoy/2b002a49921831ab0dc05415616a1652/blog-misleading-gun-deaths-graph.jpeg)|![](https://photos1.blogger.com/blogger/5757/110/1600/macgraph.jpg)|\n",
    "|-|-|\n",
    "| | |\n",
    "\n",
    "</details>    \n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Current Course Project Capability Level</u></summary>\n",
    "   \n",
    "### Current Course Project Capability Level\n",
    "    \n",
    "**Remember to abide by the [data use agreement](https://static1.squarespace.com/static/60283c2e174c122f8ebe0f39/t/6239c284d610f76fed5a2e69/1647952517436/Data+Use+Agreement+for+the+Canadian+Social+Connection+Survey.pdf) at all times.**\n",
    "\n",
    "Information about the course project is available on the course github repo [here](https://github.com/pointOfive/stat130chat130/tree/main/CP), including a draft [course project specfication](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F23_course_project_specification.ipynb) (subject to change). \n",
    "- The Week 01 HW introduced [STA130F24_CourseProject.ipynb](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F24_CourseProject.ipynb), and the [available variables](https://drive.google.com/file/d/1ISVymGn-WR1lcRs4psIym2N3or5onNBi/view). \n",
    "- Please do not download the [data](https://drive.google.com/file/d/1mbUQlMTrNYA7Ly5eImVRBn16Ehy9Lggo/view) accessible at the bottom of the [CSCS](https://casch.org/cscs) webpage (or the course github repo) multiple times.\n",
    "    \n",
    "At this point in the course you should be able to create a `for` loop to iterate through and provide **visualizations** of some of the interesting columns in the course project data\n",
    "\n",
    "1. Create a `for` loop with a **conditional logic structure** that appropriately controls the kind of visualization that gets made for a given column of data based on its data type\n",
    "\n",
    "*Being able run your code with different subsets (of different types) of columns demonstrates the desirability of the programming design principle of \"polymorphism\" (which means \"many uses\") which states that code is best when it's \"resuable\" for different purposes... such as automatically providing the appropriate visualizations as interest in different variables dynamically changes...* \n",
    "    \n",
    "</details>            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
